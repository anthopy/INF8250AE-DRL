{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db80e61c",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653edb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Head:\n",
      "         Date     BTC-USD         SPY        XLB        XLE        XLF  \\\n",
      "0  2015-10-09  243.931000  170.126617  36.063808  45.904068  15.627900   \n",
      "1  2015-10-12  245.307999  170.287201  35.753407  45.311443  15.641348   \n",
      "\n",
      "         XLI        XLK        XLP       XLRE        XLU        XLV  \\\n",
      "0  44.584923  36.932663  37.893860  21.381659  31.748842  57.945053   \n",
      "1  44.593281  36.977180  37.993462  21.516359  32.031609  58.097439   \n",
      "\n",
      "         XLY    VIX  \n",
      "0  69.639816  17.08  \n",
      "1  69.971359  16.17  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Minimalist dataset, stock values time series\n",
    "df = pd.read_csv(\"data/dataset_full.csv\")\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea534d4",
   "metadata": {},
   "source": [
    "# Environment Setup, Continuous case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbece44e",
   "metadata": {},
   "source": [
    "Env setup, reward, state_space definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Box(-1.0, 1.0, (14,), float32)\n",
      "State Space: Box(-inf, inf, (79,), float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from portfolio_env import PortfolioEnv\n",
    "\n",
    "# reward function\n",
    "def reward_log_return(env):\n",
    "    \"\"\"\n",
    "    R = ln(V_t / V_{t-1})\n",
    "    \"\"\"\n",
    "    if len(env.history['portfolio_value']) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    curr_val = env.history['portfolio_value'][-1]\n",
    "    prev_val = env.history['portfolio_value'][-2]\n",
    "    return np.log(curr_val / prev_val)\n",
    "\n",
    "# state function (enables setting the form of the states we want)\n",
    "def state_fn(env):\n",
    "    \"\"\"\n",
    "    State = [Price Returns (window), Current Weights]\n",
    "    \"\"\"\n",
    "    start_idx = env.current_step - env.window_size\n",
    "    if start_idx < 0: \n",
    "        start_idx = 0\n",
    "        \n",
    "    raw_window = env.df.iloc[start_idx : env.current_step + 1][env.asset_names].values\n",
    "    \n",
    "    returns = np.diff(raw_window, axis=0) / raw_window[:-1]\n",
    "    \n",
    "    # Flatten the matrix\n",
    "    # If we use a CNN in the future, we might keep the 2D shape.\n",
    "    flat_returns = returns.flatten()\n",
    "    \n",
    "    state = np.concatenate([flat_returns, env.weights])\n",
    "    \n",
    "    return state\n",
    "\n",
    "# dummy policy for test\n",
    "def uniform_policy(obs):\n",
    "    return np.ones(6) # give the same weight to each action\n",
    "\n",
    "\n",
    "# Env initialization for continuous state space/ action space\n",
    "env = PortfolioEnv(\n",
    "    df=df,\n",
    "    reward_fn=reward_log_return,\n",
    "    state_fn=state_fn,\n",
    "    initial_amount=1, # Initial amount of cash, doesn't really matter for the RL algo\n",
    "    window_size=5 # number of lags (passed-time steps) to pass to the observation function\n",
    ")\n",
    "\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"State Space: {env.state_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54800372",
   "metadata": {},
   "source": [
    "Continuous test: Running a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8388a5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 250, Value: 13103.76, Action Index: 110371\n",
      "Step: 500, Value: 18221.25, Action Index: 677561\n",
      "Step: 750, Value: 22226.74, Action Index: 628539\n",
      "Step: 1000, Value: 24669.34, Action Index: 1372584\n",
      "Step: 1250, Value: 37278.85, Action Index: 705054\n",
      "Step: 1500, Value: 47976.74, Action Index: 1048617\n",
      "Step: 1750, Value: 39879.13, Action Index: 1506336\n",
      "Step: 2000, Value: 45631.09, Action Index: 1520775\n",
      "Step: 2250, Value: 60133.61, Action Index: 912525\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset(\n",
    "    options={\n",
    "        #'start_date': '2021-01-04', # optionally pass the starting date\n",
    "    #'episode_length': 126       # optionally pass the episode lenght\n",
    "})\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    # 1. Get random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # 2. Step\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if env.current_step % 250 == 0:\n",
    "        print(f\"Step: {env.current_step}, Value: {env.portfolio_value:.2f}, Action Index: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c5c22",
   "metadata": {},
   "source": [
    "# Environment Setup, Discrete case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc2e0b",
   "metadata": {},
   "source": [
    "Env setup, reward, state_space definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a391acb",
   "metadata": {},
   "source": [
    "The discrete case is not particularly well suited for this task, explaning the lack of usage of Q-Learning / SARSA in finance. \n",
    "\n",
    "Indeed, the number of possible actions grows exponentially with the number of Stocks: (n_bins)**(n_stocks). \n",
    "With 10 bins, 10 stocks, that's 1e11 possible actions. Way too big for any DNN to learn.\n",
    "\n",
    "\n",
    "A work-around to limit that is to change the action space in the following way: for each action we consider selling, buying or keeping a fixed portion of the stock (ex 5%). With this workaround we have: 3**(n_stocks) actions.\n",
    "\n",
    "For 10 actions, that's 3e10 = 5,9e4 possible actions (still a lot but considerably 1e7 times less). The price to pay is that we have less precision and rapidity in the change of portfolio weights.\n",
    "\n",
    "We will likely have to start by considering a small number of stocks to study the performances and we can then iterate to a larger set of stocks if the computational power / quantity of data allows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aee8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Mode: Created 1594323 unique portfolio shift actions.\n",
      "Action Space: Discrete(1594323)\n",
      "State Space: MultiDiscrete([10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "#reward function\n",
    "def reward_log_return(env):\n",
    "    \"\"\"R = ln(V_t / V_{t-1})\"\"\"\n",
    "    if len(env.history['portfolio_value']) < 2:\n",
    "        return 0.0\n",
    "    curr_val = env.history['portfolio_value'][-1]\n",
    "    prev_val = env.history['portfolio_value'][-2]\n",
    "    return np.log(curr_val / prev_val)\n",
    "\n",
    "# state function (enables setting the form of the states we want)\n",
    "def state_fn(env):\n",
    "    \"\"\"\n",
    "    State = [Price Returns (window), Current Weights]\n",
    "    \"\"\"\n",
    "    start_idx = env.current_step - env.window_size\n",
    "    if start_idx < 0: \n",
    "        start_idx = 0\n",
    "        \n",
    "    raw_window = env.df.iloc[start_idx : env.current_step + 1][env.asset_names].values\n",
    "    \n",
    "    returns = np.diff(raw_window, axis=0) / raw_window[:-1]\n",
    "    \n",
    "    # Flatten the matrix\n",
    "    # If we use a CNN in the future, we might keep the 2D shape.\n",
    "    flat_returns = returns.flatten()\n",
    "    \n",
    "    state = np.concatenate([flat_returns, env.weights])\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "# init discrete env\n",
    "env = PortfolioEnv(\n",
    "    df=df,\n",
    "    reward_fn=reward_log_return,\n",
    "    state_fn=state_fn,\n",
    "    initial_amount=10000,\n",
    "    window_size=5,\n",
    "    \n",
    "    # -------------------- Discrete settings !---------------------------\n",
    "    action_space_type='Discrete', # Action is an Integer index\n",
    "    state_space_type='Discrete',  # State is mapped to Bins\n",
    "    \n",
    "    n_bins=10, # High bin count helps capture small return movements, but considerably increase comp/memory costs\n",
    "    \n",
    "    # Limits: \n",
    "    # Low: -0.2 (To capture negative returns down to -20%)\n",
    "    # High: 0.2 (To capture positive returns down to +20%)\n",
    "    state_space_lim=(-0.2, +0.2), \n",
    "    \n",
    "    # Step size: How much we shift weights per action (e.g. 5%)\n",
    "    step_size=0.05\n",
    ")\n",
    "\n",
    "print(f\"Action Space: {env.action_space}\") #3**(len(list(df.columns))-1) !!\n",
    "print(f\"State Space: {env.state_space}\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cb626",
   "metadata": {},
   "source": [
    "Discrete test: Running a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ed9a42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Discrete State (Bins): [5 5 4 4 5 5 5 5 5 5 5 5 3 5 4 4 4 4 4 4 4 4 4 4 4 7 5 4 5 5 4 4 4 4 5 4 4\n",
      " 4 5 5 5 5 5 5 5 5 5 4 5 5 5 2 5 5 5 5 5 4 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 9]\n",
      "Step: 250, Value: 10718.11, Action Index: 678621\n",
      "Step: 500, Value: 14038.59, Action Index: 1257877\n",
      "Step: 750, Value: 19197.87, Action Index: 37322\n",
      "Step: 1000, Value: 22460.21, Action Index: 567157\n",
      "Step: 1250, Value: 38565.53, Action Index: 1512738\n",
      "Step: 1500, Value: 56328.80, Action Index: 324618\n",
      "Step: 1750, Value: 48801.01, Action Index: 1576784\n",
      "Step: 2000, Value: 53061.93, Action Index: 507774\n",
      "Step: 2250, Value: 69143.06, Action Index: 478076\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "print(f\"\\nInitial Discrete State (Bins): {obs}\")\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    # 1. Get Discrete Action (Integer)\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # 2. Step\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if env.current_step % 250 == 0:\n",
    "        print(f\"Step: {env.current_step}, Value: {env.portfolio_value:.2f}, Action Index: {action}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
